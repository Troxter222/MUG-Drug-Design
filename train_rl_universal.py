import torch
import torch.nn as nn
import torch.optim as optim
import numpy as np
import json
import os
import selfies as sf
from rdkit import Chem, rdBase
from rdkit.Chem import Descriptors, Lipinski, AllChem

# Imports
from app.config import Config
from app.core.transformer_model import MoleculeTransformer
from app.core.vocab import Vocabulary
from app.services.chemistry import ChemistryService
from app.services.toxicity import ToxicityService

rdBase.DisableLog('rdApp.*')

# --- CONFIGURATION ---
class UniversalConfig:
    # 1. –ù–∞—á–∏–Ω–∞–µ–º —Å —Ç–≤–æ–µ–π –ª—É—á—à–µ–π —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏ (Neuro V2)
    # –ö–æ–≥–¥–∞ –æ–±—É—á–∏—à—å 14–ú, –ø—Ä–æ—Å—Ç–æ –ø–æ–º–µ–Ω—è–µ—à—å —ç—Ç–æ—Ç –ø—É—Ç—å –Ω–∞ –Ω–æ–≤—É—é –º–æ–¥–µ–ª—å!
    BASE_MODEL_PATH = "checkpoints_rl_finetuned/mug_rl_neuro_v2.pth"
    
    # –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é (—Ç–æ—Ç –∂–µ, —á—Ç–æ –∏ —É –º–æ–¥–µ–ª–∏)
    VOCAB_PATH = "dataset/processed/vocab_transformer.json"
    
    SAVE_DIR = "checkpoints_rl_universal"
    
    # –ü–∞—Ä–∞–º–µ—Ç—Ä—ã RL
    STEPS = 600          # –ß—É—Ç—å –¥–æ–ª—å—à–µ, —á—Ç–æ–±—ã –ø–µ—Ä–µ—É—á–∏—Ç—å –Ω–∞ –∫—Ä—É–ø–Ω—ã–µ –º–æ–ª–µ–∫—É–ª—ã
    BATCH_SIZE = 64      
    LEARNING_RATE = 1e-5 
    SIGMA = 50           # –°–∏–ª–∞ –Ω–∞–≥—Ä–∞–¥—ã
    KL_COEF = 0.15       # –£–¥–µ—Ä–∂–∏–≤–∞–µ–º —Å–∏–Ω—Ç–∞–∫—Å–∏—Å
    
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- HELPERS ---
def load_checkpoint_safe(model, path):
    print(f"üîÑ Loading weights: {path}")
    state_dict = torch.load(path, map_location=UniversalConfig.DEVICE)
    new_state_dict = {}
    for k, v in state_dict.items():
        if 'fc_z' in k: new_state_dict[k.replace('fc_z', 'fc_latent_to_hidden')] = v
        else: new_state_dict[k] = v
    model.load_state_dict(new_state_dict, strict=False)
    return model

# --- PROXY BINDING CALCULATOR ---
def calculate_proxy_affinity(mol):
    """
    –ë—ã—Å—Ç—Ä–∞—è –æ—Ü–µ–Ω–∫–∞ —ç–Ω–µ—Ä–≥–∏–∏ —Å–≤—è–∑—ã–≤–∞–Ω–∏—è –±–µ–∑ –∑–∞–ø—É—Å–∫–∞ Vina (–¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –æ–±—É—á–µ–Ω–∏—è).
    –û—Å–Ω–æ–≤–∞–Ω–∞ –Ω–∞ —Ñ–∏–∑–∏–∫–µ: –í–µ—Å + –í–æ–¥–æ—Ä–æ–¥–Ω—ã–µ —Å–≤—è–∑–∏ + –†–æ—Ç–æ—Ä—ã.
    """
    mw = Descriptors.MolWt(mol)
    hb = Lipinski.NumHDonors(mol) + Lipinski.NumHAcceptors(mol)
    rotors = Lipinski.NumRotatableBonds(mol)
    aromatic = len(mol.GetSubstructMatches(Chem.MolFromSmarts("a")))
    
    # –ë–∞–∑–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞: -0.025 * MW (—á–µ–º —Ç—è–∂–µ–ª–µ–µ, —Ç–µ–º –ª—É—á—à–µ, –¥–æ –ø—Ä–µ–¥–µ–ª–∞)
    # –ë–æ–Ω—É—Å –∑–∞ –∞—Ä–æ–º–∞—Ç–∏–∫—É (pi-stacking)
    score = - (mw * 0.025) - (aromatic * 0.1)
    
    # –®—Ç—Ä–∞—Ñ –∑–∞ –≥–∏–±–∫–æ—Å—Ç—å (—ç–Ω—Ç—Ä–æ–ø–∏—è)
    score += (rotors * 0.15)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º "–º–µ—á—Ç—ã"
    return max(-13.0, min(-4.0, score))

# --- ADVANCED REWARD FUNCTION ---
class UniversalReward:
    def __init__(self):
        self.tox = ToxicityService()
    
    def get_reward(self, smiles_list):
        rewards = []
        for smi in smiles_list:
            score = 0.0
            try:
                mol = Chem.MolFromSmiles(smi)
                if not mol or mol.GetNumAtoms() < 12: 
                    rewards.append(-2.0) # –£—Å–∏–ª–∏–≤–∞–µ–º —à—Ç—Ä–∞—Ñ –∑–∞ –º–µ–ª–æ—á—å
                    continue
                
                props = ChemistryService.analyze_properties(mol)
                mw = props['mw']
                
                # --- 1. –ü–†–Ø–ú–ê–Ø –¢–Ø–ì–ê –ö –í–ï–°–£ (Target: 400 Da) ---
                # –î–∞–µ–º –±–æ–Ω—É—Å, –∫–æ—Ç–æ—Ä—ã–π —Ä–∞—Å—Ç–µ—Ç –≤–º–µ—Å—Ç–µ —Å –≤–µ—Å–æ–º –¥–æ 450
                if mw < 350:
                    score += (mw / 70.0) # –ú–æ–ª–µ–∫—É–ª–∞ 280 Da –¥–∞—Å—Ç +4.0
                elif 350 <= mw <= 500:
                    score += 8.0 # –û–≥—Ä–æ–º–Ω—ã–π –¥–∂–µ–∫–ø–æ—Ç –∑–∞ –∏–¥–µ–∞–ª—å–Ω—ã–π –≤–µ—Å
                
                # --- 2. –°–ò–õ–ê –°–í–Ø–ó–ò (Affinity) ---
                proxy_affinity = calculate_proxy_affinity(mol)
                # –≠–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–∞—è –Ω–∞–≥—Ä–∞–¥–∞: –∑–∞ –∫–∞–∂–¥—ã–π —à–∞–≥ –∫ -8.0 –¥–∞–µ–º –±–æ–ª—å—à–µ
                if proxy_affinity < -8.5:
                    score += 15.0 # –§–∞–Ω—Ç–∞—Å—Ç–∏—á–µ—Å–∫–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                elif proxy_affinity < -7.5:
                    score += 8.0
                elif proxy_affinity < -6.5:
                    score += 4.0
                elif proxy_affinity < -5.5:
                    score += 1.0

                # --- 3. –§–ê–†–ú–ê–ö–û–§–û–†–ù–´–ï –ì–†–£–ü–ü–´ ---
                # –õ–µ–∫–∞—Ä—Å—Ç–≤–∞ –ª—é–±—è—Ç —Ñ—Ç–æ—Ä (F) –∏ –∞–∑–æ—Ç–Ω—ã–µ —Ü–∏–∫–ª—ã
                if any(atom.GetSymbol() == 'F' for atom in mol.GetAtoms()):
                    score += 1.5
                
                # --- 4. –®–¢–†–ê–§ –ó–ê –ü–õ–û–•–£–Æ –•–ò–ú–ò–Æ ---
                if props['qed'] < 0.3:
                    score -= 3.0 # –ú–æ–ª–µ–∫—É–ª–∞ –Ω–µ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å "–≥—Ä—è–∑–Ω–æ–π"
                
                risks = self.tox.predict(mol)
                if any("High" in r for r in risks): 
                    score -= 5.0 

            except:
                score = -1.0
            
            rewards.append(score)
        return torch.tensor(rewards, device=UniversalConfig.DEVICE)
    
# --- TRAINER ---
def train_universal():
    # 1. Init
    try:
        with open(UniversalConfig.VOCAB_PATH, 'r') as f: 
            chars = json.load(f)
        vocab = Vocabulary(chars)
    except Exception:
        # Fallback to create dummy vocab to init model (size will be fixed by checkpoint)
        vocab = Vocabulary(['<pad>']*100) 

    # 2. Get Real Vocab Size from Checkpoint
    ckpt = torch.load(UniversalConfig.BASE_MODEL_PATH, map_location=UniversalConfig.DEVICE)
    real_size = ckpt['embedding.weight'].shape[0]
    print(f"üß† Model Vocab Size: {real_size}")
    
    # 3. Models
    agent = MoleculeTransformer(real_size, 256, 8, 4, 4, 1024, 128).to(UniversalConfig.DEVICE)
    prior = MoleculeTransformer(real_size, 256, 8, 4, 4, 1024, 128).to(UniversalConfig.DEVICE)
    
    load_checkpoint_safe(agent, UniversalConfig.BASE_MODEL_PATH)
    load_checkpoint_safe(prior, UniversalConfig.BASE_MODEL_PATH)
    prior.eval()
    
    optimizer = optim.Adam(agent.parameters(), lr=UniversalConfig.LEARNING_RATE)
    scorer = UniversalReward()
    
    print("\nüöÄ Starting UNIVERSAL RL (Affinity + Safety)...")
    
    # 4. Loop
    for step in range(UniversalConfig.STEPS):
        agent.train()
        
        # Sample
        with torch.no_grad():
            seqs = agent.sample(UniversalConfig.BATCH_SIZE, UniversalConfig.DEVICE, vocab, max_len=120, temperature=1.3)
        
        # Decode & Reward
        smiles_batch = []
        for s in seqs.cpu().numpy():
            try: 
                smiles_batch.append(sf.decoder(vocab.decode(s)) or "")
            except Exception: 
                smiles_batch.append("")
            
        rewards = scorer.get_reward(smiles_batch)
        
        # Loss
        inp = seqs.t()
        enc, dec = inp, inp[:-1, :]
        tgt = inp[1:, :]
        
        logits_ag, _, _ = agent(enc, dec)
        log_p_ag = logits_ag.log_softmax(-1).gather(2, tgt.unsqueeze(2)).squeeze(2).sum(0)
        
        with torch.no_grad():
            logits_pr, _, _ = prior(enc, dec)
            log_p_pr = logits_pr.log_softmax(-1).gather(2, tgt.unsqueeze(2)).squeeze(2).sum(0)
            
        loss = - ((rewards + UniversalConfig.KL_COEF * (log_p_pr - log_p_ag)) * log_p_ag).mean()
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(agent.parameters(), 1.0)
        optimizer.step()
        
        if step % 20 == 0:
            avg_r = rewards.mean().item()
            print(f"Step {step} | Avg Reward: {avg_r:.4f}")
            valid = [s for s in smiles_batch if len(s) > 5]
            if valid: 
                mol = Chem.MolFromSmiles(valid[0])
                aff = calculate_proxy_affinity(mol) if mol else 0
                print(f"   Sample: {valid[0]}")
                print(f"   Proxy Affinity: {aff:.2f} kcal/mol (Target < -8.0)")

    # Save
    os.makedirs(UniversalConfig.SAVE_DIR, exist_ok=True)
    save_path = f"{UniversalConfig.SAVE_DIR}/mug_universal_v1.pth"
    torch.save(agent.state_dict(), save_path)
    print(f"\n‚úÖ UNIVERSAL Model Saved: {save_path}")

if __name__ == "__main__":
    train_universal()