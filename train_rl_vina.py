import torch
import torch.optim as optim
import numpy as np
import json
import os
import selfies as sf
from rdkit import Chem, rdBase
from concurrent.futures import ThreadPoolExecutor # –î–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–≥–æ –¥–æ–∫–∏–Ω–≥–∞

# Imports
from app.config import Config
from app.core.transformer_model import MoleculeTransformer
from app.core.vocab import Vocabulary
from app.services.chemistry import ChemistryService
from app.services.biology import BiologyService # –¢–µ–ø–µ—Ä—å –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ä–µ–∞–ª—å–Ω—ã–π –∫–ª–∞—Å—Å
from app.services.toxicity import ToxicityService

rdBase.DisableLog('rdApp.*')

# --- CONFIG FOR VINA TRAINING ---
class VinaRLConfig:
    # –ë–∞–∑–∞: –±–µ—Ä–µ–º –Ω–∞—à—É –ª—É—á—à—É—é Neuro V2 –∏–ª–∏ Universal
    BASE_MODEL_PATH = "checkpoints_rl_finetuned/mug_rl_neuro_v2.pth"
    VOCAB_PATH = "dataset/processed/vocab_transformer.json"
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –æ—Ç–¥–µ–ª—å–Ω—É—é –ø–∞–ø–∫—É
    SAVE_DIR = "checkpoints_vina_egfr"
    
    # –ú–µ–Ω—å—à–µ —à–∞–≥–æ–≤, —Ç–∞–∫ –∫–∞–∫ –∫–∞–∂–¥—ã–π —à–∞–≥ –æ—á–µ–Ω—å "—É–º–Ω—ã–π"
    STEPS = 200          
    BATCH_SIZE = 32      # –£–º–µ–Ω—å—à–∏–ª –±–∞—Ç—á, —á—Ç–æ–±—ã CPU –Ω–µ –∑–∞—Ö–ª–µ–±–Ω—É–ª—Å—è
    LEARNING_RATE = 5e-6 # –û—á–µ–Ω—å –∞–∫–∫—É—Ä–∞—Ç–Ω–æ –º–µ–Ω—è–µ–º –≤–µ—Å–∞
    
    # –í–ê–ñ–ù–û: –¶–µ–ª—å –¥–ª—è –¥–æ–∫–∏–Ω–≥–∞ (–¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ targets_config.json)
    TARGET_CATEGORY = "lung" # EGFR
    TARGET_KEY = "lung"      # –ö–ª—é—á –¥–ª—è BiologyService

    KL_COEF = 0.2
    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# --- PHYSICS-BASED REWARD ---
class VinaReward:
    def __init__(self):
        self.bio = BiologyService()
        self.tox = ToxicityService()
        self.chem = ChemistryService()
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –≥–æ—Ç–æ–≤–∞ –ª–∏ Vina
        if not self.bio.vina_ready:
            raise RuntimeError("‚ùå Vina.exe not found! Cannot train using physics.")
            
    def calculate_single_score(self, smi):
        """–°—á–∏—Ç–∞–µ—Ç –Ω–∞–≥—Ä–∞–¥—É –¥–ª—è –æ–¥–Ω–æ–π –º–æ–ª–µ–∫—É–ª—ã (–±—É–¥–µ—Ç –∑–∞–ø—É—â–µ–Ω–æ –≤ –ø–æ—Ç–æ–∫–µ)."""
        if not smi: return 0.0
        
        try:
            mol = Chem.MolFromSmiles(smi)
            if not mol: return 0.0
            
            # 1. –°–≤–æ–π—Å—Ç–≤–∞
            props = self.chem.analyze_properties(mol)
            
            # –ï—Å–ª–∏ –º–æ–ª–µ–∫—É–ª–∞ —Å–æ–≤—Å–µ–º "–∫—Ä–∏–≤–∞—è" (—Å–ª–∏—à–∫–æ–º –º–µ–ª–∫–∞—è/–æ–≥—Ä–æ–º–Ω–∞—è),
            # –¥–∞–∂–µ –Ω–µ —Ç—Ä–∞—Ç–∏–º –≤—Ä–µ–º—è CPU –Ω–∞ –¥–æ–∫–∏–Ω–≥.
            if props['mw'] < 200 or props['mw'] > 650:
                return -0.5
            
            # 2. REAL DOCKING (–°–∞–º–æ–µ –≤–∞–∂–Ω–æ–µ)
            # –≠—Ç–æ –≤–µ—Ä–Ω–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–µ –∫–∫–∞–ª/–º–æ–ª—å (–Ω–∞–ø—Ä–∏–º–µ—Ä, -8.5)
            affinity = self.bio.dock_molecule(mol, VinaRLConfig.TARGET_CATEGORY)
            
            # 3. –ü—Ä–µ–≤—Ä–∞—â–∞–µ–º —ç–Ω–µ—Ä–≥–∏—é –≤ –Ω–∞–≥—Ä–∞–¥—É (Score)
            # –ù–∞–º –Ω—É–∂–Ω–æ -9.0 –∏ –Ω–∏–∂–µ.
            # –ï—Å–ª–∏ -6.0 -> Score 0
            # –ï—Å–ª–∏ -9.0 -> Score 3.0
            # –ï—Å–ª–∏ -11.0 -> Score 5.0
            docking_reward = max(0, -(affinity + 6.0)) 
            
            # 4. –®—Ç—Ä–∞—Ñ—ã
            risks = self.tox.predict(mol)
            tox_penalty = 1.0 if any("High" in r for r in risks) else 0.0
            
            # –ò—Ç–æ–≥–æ–≤–∞—è —Ñ–æ—Ä–º—É–ª–∞
            final_score = docking_reward - tox_penalty + props['qed']
            
            return final_score
            
        except Exception:
            return 0.0

    def get_reward_batch(self, smiles_list):
        """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –∑–∞–ø—É—Å–∫ Vina –¥–ª—è –≤—Å–µ–≥–æ –±–∞—Ç—á–∞"""
        print(f"   üß¨ Docking {len(smiles_list)} molecules in parallel...", end="\r")
        
        with ThreadPoolExecutor(max_workers=os.cpu_count()) as executor:
            rewards = list(executor.map(self.calculate_single_score, smiles_list))
            
        return torch.tensor(rewards, device=VinaRLConfig.DEVICE)

# --- HELPERS ---
def load_adapter(model, path):
    print(f"üîÑ Adapter load: {path}")
    state = torch.load(path, map_location=VinaRLConfig.DEVICE)
    new_state = {}
    for k, v in state.items():
        if 'fc_z' in k: new_state[k.replace('fc_z', 'fc_latent_to_hidden')] = v
        else: new_state[k] = v
    model.load_state_dict(new_state, strict=False)

# --- TRAINING LOOP ---
def train_vina():
    os.makedirs(VinaRLConfig.SAVE_DIR, exist_ok=True)
    
    # Init
    with open(VinaRLConfig.VOCAB_PATH) as f: vocab = Vocabulary(json.load(f))
    
    # Models
    vocab_size = 29 # –ò–ª–∏ len(vocab), –ø—Ä–æ–≤–µ—Ä—å –ø–æ –ø—Ä–µ–¥—ã–¥—É—â–µ–º—É —É—Ä–æ–∫—É!
    agent = MoleculeTransformer(vocab_size, 256, 8, 4, 4, 1024, 128).to(VinaRLConfig.DEVICE)
    prior = MoleculeTransformer(vocab_size, 256, 8, 4, 4, 1024, 128).to(VinaRLConfig.DEVICE)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤–µ—Å–∞ (—É —Ç–µ–±—è –º–æ–∂–µ—Ç –±—ã—Ç—å 29 –∏–ª–∏ 118 —Ç–æ–∫–µ–Ω–æ–≤, –ø—Ä–æ–≤–µ—Ä—å vocab_size!)
    # –ï—Å–ª–∏ –ø–∞–¥–∞–µ—Ç –Ω–∞ mismatch, —Å—Ç–∞–≤—å vocab_size=29 –≤—Ä—É—á–Ω—É—é –∫–∞–∫ –≤ –ø—Ä–æ—à–ª–æ–º —Ñ–∏–∫—Å–µ
    try:
        load_adapter(agent, VinaRLConfig.BASE_MODEL_PATH)
        load_adapter(prior, VinaRLConfig.BASE_MODEL_PATH)
    except:
        print("‚ö†Ô∏è Vocab mismatch suspected. Assuming old vocab size 29.")
        agent = MoleculeTransformer(29, 256, 8, 4, 4, 1024, 128).to(VinaRLConfig.DEVICE)
        prior = MoleculeTransformer(29, 256, 8, 4, 4, 1024, 128).to(VinaRLConfig.DEVICE)
        load_adapter(agent, VinaRLConfig.BASE_MODEL_PATH)
        load_adapter(prior, VinaRLConfig.BASE_MODEL_PATH)

    prior.eval()
    optimizer = optim.Adam(agent.parameters(), lr=VinaRLConfig.LEARNING_RATE)
    scorer = VinaReward()
    
    print(f"\nüöÄ Starting PHYSICS-BASED RL for target: {VinaRLConfig.TARGET_CATEGORY}")
    print("‚ö†Ô∏è This will take time. Watch the reward grow!")

    for step in range(VinaRLConfig.STEPS):
        agent.train()
        
        # 1. Sample
        with torch.no_grad():
            seqs = agent.sample(VinaRLConfig.BATCH_SIZE, VinaRLConfig.DEVICE, vocab, max_len=120, temperature=1.0)
        
        # 2. Decode
        smiles_batch = []
        for s in seqs.cpu().numpy():
            try: smiles_batch.append(sf.decoder(vocab.decode(s)) or "")
            except: smiles_batch.append("")
            
        # 3. REAL VINA SCORING (Slow part)
        rewards = scorer.get_reward_batch(smiles_batch)
        
        # 4. PPO Loss
        inp = seqs.t()
        logits_ag, _, _ = agent(inp, inp[:-1, :])
        
        with torch.no_grad():
            logits_pr, _, _ = prior(inp, inp[:-1, :])
            
        # –í—ã–±–∏—Ä–∞–µ–º –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
        tgt = inp[1:, :]
        log_p_ag = logits_ag.log_softmax(-1).gather(2, tgt.unsqueeze(2)).squeeze(2).sum(0)
        log_p_pr = logits_pr.log_softmax(-1).gather(2, tgt.unsqueeze(2)).squeeze(2).sum(0)
        
        loss = - ((rewards + VinaRLConfig.KL_COEF * (log_p_pr - log_p_ag)) * log_p_ag).mean()
        
        optimizer.zero_grad()
        loss.backward()
        torch.nn.utils.clip_grad_norm_(agent.parameters(), 1.0)
        optimizer.step()
        
        avg_r = rewards.mean().item()
        print(f"Step {step+1}/{VinaRLConfig.STEPS} | Avg Reward: {avg_r:.4f} | Loss: {loss.item():.2f}")
        
        # Save every 50 steps
        if (step+1) % 50 == 0:
            path = f"{VinaRLConfig.SAVE_DIR}/mug_egfr_hunter_step{step+1}.pth"
            torch.save(agent.state_dict(), path)
            print(f"üíæ Checkpoint: {path}")

    # Final Save
    final_path = f"{VinaRLConfig.SAVE_DIR}/mug_egfr_hunter_final.pth"
    torch.save(agent.state_dict(), final_path)
    print(f"‚úÖ Training Complete! Model saved to {final_path}")

if __name__ == "__main__":
    train_vina()