import torch
import glob
import json
import os
import pandas as pd
import numpy as np
import selfies as sf
from tqdm import tqdm
from rdkit import Chem, DataStructs
from rdkit.Chem import Descriptors, Crippen, Lipinski, GraphDescriptors, AllChem

# –ò–º–ø–æ—Ä—Ç—ã –ø—Ä–æ–µ–∫—Ç–∞
from app.core.engine import MolecularVAE
from app.core.transformer_model import MoleculeTransformer
from app.core.vocab import Vocabulary
from app.services.chemistry import ChemistryService

# --- –ù–ê–°–¢–†–û–ô–ö–ò ---
SEARCH_DIRS = [
    "checkpoints", 
    "checkpoints_selfies", 
    "checkpoints_rl_ultimate", 
    "checkpoints_transformer", 
    "checkpoints_rl_transformer"
]
TRAIN_DATA_PATH = "data/processed/train_selfies.csv"
VOCAB_GRU = "data/processed/vocab_selfies.json"
VOCAB_TRANS = "dataset/processed/vocab_transformer.json" # –ü—É—Ç—å –∫ —Å–ª–æ–≤–∞—Ä—é —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞

DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
N_SAMPLES = 500 # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –º–æ–ª–µ–∫—É–ª –¥–ª—è —Ç–µ—Å—Ç–∞

class BenchmarkEngine:
    def __init__(self):
        print("‚è≥ –ó–∞–≥—Ä—É–∑–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ –Ω–æ–≤–∏–∑–Ω—ã...")
        try:
            df = pd.read_csv(TRAIN_DATA_PATH)
            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–µ 100–∫ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏, –∏–ª–∏ –≤—Å–µ –µ—Å–ª–∏ –ø–∞–º—è—Ç–∏ –º–Ω–æ–≥–æ
            self.train_smiles = set()
            for s in tqdm(df['SELFIES'][:100000]):
                try: self.train_smiles.add(sf.decoder(s))
                except: pass
            print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(self.train_smiles)} —É–Ω–∏–∫–∞–ª—å–Ω—ã—Ö –º–æ–ª–µ–∫—É–ª –∏–∑ train.")
        except:
            print("‚ö†Ô∏è –¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω. Novelty –±—É–¥–µ—Ç 100%.")
            self.train_smiles = set()

    def get_model_type(self, path):
        if "transformer" in path.lower():
            return "transformer", VOCAB_TRANS
        return "gru", VOCAB_GRU

    def load_model(self, path):
        model_type, vocab_path = self.get_model_type(path)
        
        try:
            with open(vocab_path, 'r') as f: chars = json.load(f)
            if '<sos>' not in chars: chars = ['<pad>', '<sos>', '<eos>'] + sorted(chars)
            vocab = Vocabulary(chars)
            
            checkpoint = torch.load(path, map_location=DEVICE)
            vocab_size = checkpoint['embedding.weight'].shape[0]
            
            # –ê–≤—Ç–æ-—Ñ–∏–∫—Å —Ä–∞–∑–º–µ—Ä–∞ —Å–ª–æ–≤–∞—Ä—è, –µ—Å–ª–∏ –Ω–µ —Å–æ–≤–ø–∞–¥–∞–µ—Ç
            if len(vocab) != vocab_size:
                # –í—Ä–µ–º–µ–Ω–Ω–æ –ø–æ–¥–º–µ–Ω—è–µ–º —Ä–∞–∑–º–µ—Ä —Å–ª–æ–≤–∞—Ä—è –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ –≤–µ—Å–æ–≤
                # (–í —Ä–µ–∞–ª—å–Ω–æ–º —Ç–µ—Å—Ç–µ —ç—Ç–æ –º–æ–∂–µ—Ç —Å–ª–æ–º–∞—Ç—å –≥–µ–Ω–µ—Ä–∞—Ü–∏—é, –Ω–æ –º—ã –ø–æ–ø—Ä–æ–±—É–µ–º)
                real_vocab_len = vocab_size
            else:
                real_vocab_len = len(vocab)

            if model_type == "transformer":
                model = MoleculeTransformer(real_vocab_len, d_model=128, nhead=4, num_encoder_layers=3, num_decoder_layers=3, latent_size=64)
            else:
                model = MolecularVAE(real_vocab_len, 64, 256, 128, 3)
                
            model.load_state_dict(checkpoint)
            model.to(DEVICE)
            model.eval()
            return model, vocab, model_type
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {path}: {e}")
            return None, None, None

    def calculate_metrics(self, smiles_list):
        valid_mols = []
        valid_smiles = []
        
        for s in smiles_list:
            if not s: continue
            m = Chem.MolFromSmiles(s)
            if m:
                valid_mols.append(m)
                valid_smiles.append(s)
        
        total = len(smiles_list)
        if total == 0: return None
        
        # 1. Validity
        validity = len(valid_mols) / total
        
        if not valid_mols:
            return {"Validity": 0.0, "Score": 0.0}

        # 2. Uniqueness
        unique_smiles = set(valid_smiles)
        uniqueness = len(unique_smiles) / len(valid_mols)
        
        # 3. Novelty
        new_mols = [s for s in unique_smiles if s not in self.train_smiles]
        novelty = len(new_mols) / len(unique_smiles)
        
        # 4. Diversity (Tanimoto)
        fps = [AllChem.GetMorganFingerprintAsBitVect(m, 2, nBits=1024) for m in valid_mols]
        if len(fps) > 1:
            divs = []
            for i in range(10): # –†–∞–Ω–¥–æ–º–Ω–∞—è –≤—ã–±–æ—Ä–∫–∞ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏
                import random
                if len(fps) < 2: break
                a, b = random.sample(fps, 2)
                divs.append(1.0 - DataStructs.TanimotoSimilarity(a, b))
            diversity = np.mean(divs)
        else:
            diversity = 0.0

        # 5. Properties
        qeds = []
        logps = []
        mws = []
        sas = []
        tox_alerts = 0
        
        for m in valid_mols:
            qeds.append(Descriptors.qed(m))
            logps.append(Crippen.MolLogP(m))
            mws.append(Descriptors.MolWt(m))
            
            # SA Score proxy
            complexity = GraphDescriptors.BertzCT(m)
            sa = (complexity - 200) / 100
            sas.append(sa)
            
            # Tox Check (Simple)
            if m.HasSubstructMatch(Chem.MolFromSmarts("[N+](=O)[O-]")) or \
               m.HasSubstructMatch(Chem.MolFromSmarts("c1ccccc1[Cl,Br,I]")):
                tox_alerts += 1
                
        avg_qed = np.mean(qeds)
        avg_sa = np.mean(sas)
        avg_tox = tox_alerts / len(valid_mols)
        
        # 6. Reward (Approximation)
        # R = QED*10 + (1-Tox)*5 + (Novelty)*5
        avg_reward = (avg_qed * 10) + ((1 - avg_tox) * 5) + (novelty * 5)
        
        return {
            "Validity": validity * 100,
            "Uniqueness": uniqueness * 100,
            "Novelty": novelty * 100,
            "Diversity": diversity,
            "QED": avg_qed,
            "SA": avg_sa,
            "LogP": np.mean(logps),
            "MW": np.mean(mws),
            "Tox_Rate": avg_tox * 100,
            "Reward": avg_reward
        }

    def run(self):
        # –ù–∞—Ö–æ–¥–∏–º –≤—Å–µ —Ñ–∞–π–ª—ã –º–æ–¥–µ–ª–µ–π
        files = []
        for d in SEARCH_DIRS:
            files.extend(glob.glob(os.path.join(d, "*.pth")))
            
        print(f"üîé –ù–∞–π–¥–µ–Ω–æ {len(files)} –º–æ–¥–µ–ª–µ–π. –ù–∞—á–∏–Ω–∞—é —Ç—É—Ä–Ω–∏—Ä...\n")
        
        report = []
        
        for f in files:
            print(f"ü§ñ Testing: {f} ...", end=" ")
            model, vocab, mtype = self.load_model(f)
            
            if not model: continue
            
            # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º
            generated_smiles = []
            with torch.no_grad():
                for _ in range(int(N_SAMPLES / 10)): # –ë–∞—Ç—á–∞–º–∏ –ø–æ 10
                    try:
                        indices = model.sample(10, DEVICE, vocab, max_len=150, temp=0.8)
                        # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, —á—Ç–æ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç sample (—Å–ø–∏—Å–æ–∫ –∏–ª–∏ —Ç–µ–Ω–∑–æ—Ä)
                        if isinstance(indices, list):
                            # –ï—Å–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–ª–æ—Å–∫–∏–π —Å–ø–∏—Å–æ–∫ (–¥–ª—è –±–∞—Ç—á–∞=1), —ç—Ç–æ –ø—Ä–æ–±–ª–µ–º–∞ –¥–ª—è –±–∞—Ç—á–∞ 10
                            # –ü—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º sample –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Ç–µ–Ω–∑–æ—Ä [batch, len]
                            pass 
                        
                        # –ö–æ—Å—Ç—ã–ª—å –¥–ª—è —É–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω–æ—Å—Ç–∏ (—Ç–∞–∫ –∫–∞–∫ sample –≤ —Ä–∞–∑–Ω—ã—Ö –≤–µ—Ä—Å–∏—è—Ö —Ä–∞–∑–Ω—ã–π)
                        # –ì–µ–Ω–µ—Ä–∏–º –ø–æ 1, –µ—Å–ª–∏ –±–∞—Ç—á –Ω–µ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –≤ sample
                        # –ù–æ –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –ø—Ä–µ–¥–ø–æ–ª–æ–∂–∏–º, —á—Ç–æ sample –ø–µ—Ä–µ–ø–∏—Å–∞–Ω –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º —Ü–∏–∫–ª
                        pass
                    except: pass
            
            # –ü—Ä–æ—Å—Ç–æ–π —Ü–∏–∫–ª –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–æ 1 (–º–µ–¥–ª–µ–Ω–Ω–æ, –Ω–æ –Ω–∞–¥–µ–∂–Ω–æ –¥–ª—è –≤—Å–µ—Ö –≤–µ—Ä—Å–∏–π –∫–æ–¥–∞)
            for _ in range(50): # 50 –º–æ–ª–µ–∫—É–ª –¥–ª—è —Ç–µ—Å—Ç–∞ (—á—Ç–æ–±—ã –±—ã—Å—Ç—Ä–æ)
                try:
                    if mtype == "transformer":
                        idx = model.sample(DEVICE, vocab, max_len=100)
                        s = vocab.decode(torch.tensor(idx))
                    else:
                        idx = model.sample(1, DEVICE, vocab, max_len=100, temp=0.8)
                        s = vocab.decode(idx.cpu().numpy()[0])
                    
                    smi = sf.decoder(s)
                    generated_smiles.append(smi)
                except: continue
                
            metrics = self.calculate_metrics(generated_smiles)
            
            if metrics:
                metrics['Model'] = os.path.basename(f)
                metrics['Type'] = mtype
                report.append(metrics)
                print(f"‚úÖ QED: {metrics['QED']:.2f} | Valid: {metrics['Validity']:.0f}%")
            else:
                print("‚ùå Fail")

        # --- –ê–ù–ê–õ–ò–ó –ò –í–´–í–û–î ---
        if not report:
            print("–ù–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤.")
            return

        df = pd.DataFrame(report)
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ "–°—É–ø–µ—Ä-–º–µ—Ç—Ä–∏–∫–µ" (MIT Score)
        # Score = QED (max) + Validity (max) - Tox (min)
        df['MIT_Score'] = (df['QED'] * 20) + (df['Validity'] / 10) - (df['Tox_Rate'] / 10) + (df['Novelty'] / 20)
        
        df = df.sort_values(by='MIT_Score', ascending=False)
        
        print("\n" + "="*80)
        print("üèÜ –†–ï–ó–£–õ–¨–¢–ê–¢–´ –¢–£–†–ù–ò–†–ê –ú–û–î–ï–õ–ï–ô MUG")
        print("="*80)
        
        print("\nü•á –¢–û–ü-3 –õ–£–ß–®–ò–• –ú–û–î–ï–õ–ò:")
        print(df[['Model', 'Type', 'MIT_Score', 'QED', 'Validity', 'Tox_Rate']].head(3).to_string(index=False))
        
        print("\nüìä –ü–û–õ–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê (–ì—Ä—É–ø–ø–∞ A - –°—Ç—Ä—É–∫—Ç—É—Ä–∞):")
        print(df[['Model', 'Validity', 'Uniqueness', 'Novelty', 'Diversity']].to_string(index=False))
        
        print("\nüíä –ü–û–õ–ù–ê–Ø –¢–ê–ë–õ–ò–¶–ê (–ì—Ä—É–ø–ø–∞ B - –§–∞—Ä–º–∞):")
        print(df[['Model', 'QED', 'SA', 'LogP', 'MW', 'Tox_Rate']].to_string(index=False))
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ
        df.to_csv("benchmark_results.csv", index=False)
        print("\nüíæ –ü–æ–ª–Ω—ã–π –æ—Ç—á–µ—Ç —Å–æ—Ö—Ä–∞–Ω–µ–Ω –≤ benchmark_results.csv")

if __name__ == "__main__":
    BenchmarkEngine().run()